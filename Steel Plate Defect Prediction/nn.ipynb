{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/200, Loss: 0.37571215629577637\n",
      "tensor([[0.0638, 0.1869, 0.1027, 0.0514, 0.0429, 0.1650, 0.4417]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1491, 0.1001, 0.0060, 0.0211, 0.0179, 0.2647, 0.4920]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2990, 0.0285, 0.0176, 0.0081, 0.0125, 0.2680, 0.4544]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0161, 0.0384, 0.8632, 0.0192, 0.0092, 0.0076, 0.1328]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0128, 0.0215, 0.9484, 0.0240, 0.0043, 0.0110, 0.1624]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1084, 0.1468, 0.0349, 0.0777, 0.0668, 0.2749, 0.3787]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0179, 0.0088, 0.8681, 0.0252, 0.0068, 0.0156, 0.2061]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1201, 0.1319, 0.0832, 0.1162, 0.1338, 0.3101, 0.2361]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0411, 0.1491, 0.0241, 0.0468, 0.0663, 0.2598, 0.3148]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0669, 0.0464, 0.0157, 0.0787, 0.0410, 0.2275, 0.4482]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[1.4575e-04, 1.1440e-05, 9.7269e-01, 1.4583e-06, 6.9789e-05, 3.6654e-05,\n",
      "         2.2372e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1170, 0.0744, 0.0527, 0.0088, 0.0091, 0.1568, 0.5543]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0111, 0.0262, 0.9082, 0.0092, 0.0081, 0.0104, 0.0602]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0820, 0.0361, 0.0040, 0.0142, 0.0158, 0.3863, 0.3007]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0481, 0.0244, 0.0022, 0.0133, 0.0148, 0.1691, 0.4216]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0756, 0.2459, 0.0512, 0.0314, 0.0291, 0.2176, 0.3649]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0959, 0.0738, 0.0056, 0.0496, 0.0448, 0.2490, 0.3395]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0451, 0.0883, 0.0020, 0.0371, 0.0225, 0.3088, 0.3729]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1225, 0.0338, 0.0037, 0.0335, 0.0541, 0.4909, 0.5208]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1591, 0.3770, 0.0516, 0.0214, 0.0497, 0.1883, 0.3287]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1226, 0.0517, 0.0166, 0.0656, 0.0675, 0.3994, 0.4766]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1634, 0.0233, 0.0072, 0.0223, 0.0310, 0.3806, 0.3798]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0476, 0.0098, 0.0007, 0.0061, 0.0107, 0.4699, 0.3725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1611, 0.1353, 0.0304, 0.0132, 0.0154, 0.1004, 0.2128]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0657, 0.0337, 0.0173, 0.0917, 0.0983, 0.2990, 0.2899]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0293, 0.0277, 0.0087, 0.0052, 0.0052, 0.4800, 0.5884]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0251, 0.0329, 0.9730, 0.0051, 0.0136, 0.0150, 0.0243]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1110, 0.0359, 0.0028, 0.0199, 0.0463, 0.4381, 0.2517]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0505, 0.0204, 0.0139, 0.0358, 0.0257, 0.1244, 0.6611]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[1.4368e-02, 8.4683e-03, 7.6997e-01, 4.6521e-04, 3.6682e-03, 2.6847e-03,\n",
      "         4.1512e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0997, 0.0165, 0.0014, 0.0094, 0.0329, 0.5997, 0.2243]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0441, 0.0450, 0.0014, 0.0191, 0.0110, 0.4496, 0.6182]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0982, 0.1193, 0.0733, 0.0667, 0.0333, 0.2667, 0.3580]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1175, 0.0369, 0.0030, 0.0281, 0.0156, 0.4632, 0.4607]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0736, 0.0313, 0.0161, 0.0313, 0.0354, 0.4973, 0.2432]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0712, 0.1285, 0.0177, 0.0435, 0.0257, 0.3311, 0.3076]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0645, 0.0223, 0.2720, 0.0009, 0.0018, 0.1474, 0.1665]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0112, 0.0124, 0.9272, 0.0247, 0.0079, 0.0186, 0.2643]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1035, 0.0602, 0.0050, 0.0615, 0.0315, 0.3830, 0.2844]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1121, 0.0764, 0.1321, 0.0666, 0.0400, 0.1457, 0.3443]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0303, 0.0118, 0.9727, 0.0104, 0.0021, 0.0036, 0.0471]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1378, 0.0571, 0.0070, 0.0182, 0.0150, 0.3027, 0.3118]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[1.7341e-03, 3.6362e-03, 9.7435e-01, 7.8103e-04, 3.8278e-04, 1.8932e-03,\n",
      "         3.9093e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0994, 0.0219, 0.0050, 0.0434, 0.0172, 0.5059, 0.2518]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1631, 0.1827, 0.0094, 0.0258, 0.0245, 0.4492, 0.2595]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0049, 0.0096, 0.9734, 0.0033, 0.0071, 0.0093, 0.0285]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0628, 0.0326, 0.0119, 0.0544, 0.0423, 0.2820, 0.4056]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0477, 0.0252, 0.2740, 0.0157, 0.0069, 0.0950, 0.5027]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0515, 0.0332, 0.0055, 0.0250, 0.0090, 0.3984, 0.3980]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0562, 0.0335, 0.7272, 0.0191, 0.0256, 0.0277, 0.0944]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0655, 0.0415, 0.0095, 0.0218, 0.0130, 0.3282, 0.4501]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0476, 0.1074, 0.0168, 0.0260, 0.0189, 0.1688, 0.3202]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1067, 0.0403, 0.0354, 0.0545, 0.0170, 0.2023, 0.3904]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0567, 0.1305, 0.0236, 0.0382, 0.0529, 0.5102, 0.3521]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0151, 0.0804, 0.8678, 0.0240, 0.0219, 0.0214, 0.2115]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0470, 0.0695, 0.0130, 0.0462, 0.0461, 0.3226, 0.3092]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0812, 0.0735, 0.0022, 0.0108, 0.0226, 0.4745, 0.3581]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0791, 0.0854, 0.0150, 0.0254, 0.0107, 0.3538, 0.3465]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1967, 0.2845, 0.0560, 0.0309, 0.0367, 0.1549, 0.4092]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1424, 0.1470, 0.1557, 0.0313, 0.0288, 0.1999, 0.2989]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0985, 0.1156, 0.0173, 0.0189, 0.0475, 0.5035, 0.4429]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0922, 0.0828, 0.0531, 0.0562, 0.0466, 0.1691, 0.4725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0582, 0.0190, 0.0270, 0.0138, 0.0337, 0.3594, 0.4410]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1825, 0.1164, 0.0097, 0.0447, 0.0831, 0.3407, 0.2479]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1156, 0.0267, 0.0045, 0.0355, 0.0190, 0.3058, 0.4639]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0815, 0.1533, 0.0630, 0.0203, 0.0311, 0.1240, 0.3052]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1230, 0.0477, 0.0023, 0.0077, 0.0290, 0.2269, 0.4358]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0816, 0.1479, 0.0474, 0.0852, 0.0936, 0.2936, 0.3692]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0818, 0.0324, 0.0135, 0.0499, 0.0675, 0.4300, 0.3891]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0236, 0.0029, 0.0009, 0.0104, 0.0056, 0.2421, 0.7591]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1294, 0.0338, 0.0135, 0.0320, 0.0705, 0.2825, 0.4239]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0983, 0.0355, 0.1725, 0.0580, 0.0249, 0.2269, 0.4699]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0760, 0.0212, 0.0627, 0.0620, 0.0330, 0.2233, 0.3947]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1383, 0.0251, 0.0368, 0.0272, 0.0189, 0.4222, 0.4714]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0890, 0.0591, 0.0207, 0.0202, 0.0231, 0.3433, 0.4063]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2133, 0.1444, 0.2392, 0.1164, 0.0639, 0.2490, 0.3464]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0022, 0.0010, 0.6366, 0.0161, 0.0010, 0.0119, 0.1586]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0685, 0.0793, 0.0046, 0.0184, 0.0261, 0.2681, 0.2864]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0280, 0.0228, 0.9469, 0.0194, 0.0121, 0.0133, 0.0360]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1603, 0.0064, 0.0129, 0.0232, 0.0333, 0.2359, 0.4081]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0412, 0.0808, 0.0024, 0.0242, 0.0217, 0.2977, 0.3966]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1370, 0.1600, 0.0297, 0.0207, 0.0282, 0.2224, 0.2837]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0373, 0.0699, 0.0025, 0.0208, 0.0105, 0.2690, 0.2983]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0237, 0.0453, 0.0448, 0.0166, 0.0193, 0.2219, 0.4447]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0902, 0.0280, 0.0087, 0.0286, 0.0234, 0.4564, 0.4136]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[4.4484e-02, 1.7269e-04, 9.3201e-01, 2.2219e-05, 3.7268e-04, 1.4435e-03,\n",
      "         1.5763e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2276, 0.1228, 0.0134, 0.0351, 0.0770, 0.2789, 0.4375]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0020, 0.0823, 0.9826, 0.0033, 0.0028, 0.0052, 0.0476]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0401, 0.0584, 0.9216, 0.0161, 0.0245, 0.0194, 0.1369]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0657, 0.0246, 0.0165, 0.0037, 0.0087, 0.5913, 0.5777]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0483, 0.0383, 0.0074, 0.0294, 0.0146, 0.3424, 0.5351]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1161, 0.1007, 0.0013, 0.0179, 0.0328, 0.2469, 0.3399]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0809, 0.0513, 0.0730, 0.0286, 0.0162, 0.2085, 0.4556]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0531, 0.0840, 0.0435, 0.0169, 0.0077, 0.1758, 0.3707]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1248, 0.1365, 0.0202, 0.0475, 0.1040, 0.1805, 0.3627]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0847, 0.0295, 0.0645, 0.0794, 0.0777, 0.1568, 0.4062]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0451, 0.0419, 0.0097, 0.0219, 0.0377, 0.2845, 0.3879]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1901, 0.0376, 0.0088, 0.0204, 0.0229, 0.1990, 0.3633]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0820, 0.3088, 0.0165, 0.0101, 0.0199, 0.0991, 0.3018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0194, 0.0056, 0.6091, 0.0303, 0.0158, 0.0407, 0.2146]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2097, 0.1047, 0.0391, 0.0083, 0.0124, 0.2270, 0.3151]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0795, 0.1076, 0.0383, 0.0163, 0.0503, 0.3160, 0.3863]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0970, 0.0202, 0.0143, 0.0072, 0.0086, 0.3641, 0.5749]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0279, 0.0495, 0.7350, 0.0053, 0.0051, 0.0126, 0.0781]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0162, 0.0122, 0.7342, 0.0031, 0.0025, 0.0410, 0.0603]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0699, 0.0907, 0.0314, 0.0510, 0.0192, 0.4207, 0.4132]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0249, 0.0126, 0.8796, 0.0066, 0.0236, 0.0208, 0.1411]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0485, 0.0231, 0.0108, 0.0085, 0.0128, 0.5343, 0.6613]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1065, 0.0993, 0.0384, 0.2746, 0.0542, 0.2990, 0.3817]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1196, 0.1074, 0.0046, 0.0042, 0.0070, 0.1844, 0.4377]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0905, 0.0417, 0.0223, 0.0531, 0.0587, 0.3763, 0.5343]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1828, 0.0232, 0.0067, 0.0161, 0.0288, 0.3542, 0.4923]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0052, 0.0041, 0.8609, 0.0020, 0.0012, 0.0105, 0.1725]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[4.9107e-02, 3.5854e-02, 5.5598e-04, 6.1064e-03, 1.8383e-02, 5.7162e-01,\n",
      "         4.8869e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0820, 0.0353, 0.0310, 0.0696, 0.0245, 0.2370, 0.3663]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0920, 0.0328, 0.0016, 0.0031, 0.0057, 0.5196, 0.4516]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0485, 0.0213, 0.0631, 0.0076, 0.0093, 0.2058, 0.4067]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1809, 0.0899, 0.0596, 0.0492, 0.0442, 0.2417, 0.3986]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0609, 0.0424, 0.0044, 0.0168, 0.0153, 0.4170, 0.3659]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0707, 0.0799, 0.0126, 0.0289, 0.0247, 0.4033, 0.2514]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0503, 0.0143, 0.0163, 0.1929, 0.0216, 0.1556, 0.3316]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0298, 0.0248, 0.0073, 0.0341, 0.0139, 0.4265, 0.3462]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1242, 0.0096, 0.0041, 0.0012, 0.0031, 0.4551, 0.4803]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0289, 0.0575, 0.0206, 0.0146, 0.0082, 0.2812, 0.3835]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0135, 0.0124, 0.9371, 0.0115, 0.0082, 0.0196, 0.0589]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1014, 0.0602, 0.0206, 0.0392, 0.0517, 0.3626, 0.3388]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0549, 0.0108, 0.0155, 0.0081, 0.0103, 0.4232, 0.5704]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1513, 0.1442, 0.1752, 0.0277, 0.0461, 0.1522, 0.2950]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2410, 0.0859, 0.0055, 0.0103, 0.0300, 0.3105, 0.2704]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1588, 0.1138, 0.0282, 0.0129, 0.0387, 0.3017, 0.2877]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0681, 0.0576, 0.0028, 0.0147, 0.0197, 0.6879, 0.3090]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1702, 0.1255, 0.0063, 0.0108, 0.0167, 0.4925, 0.4477]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1390, 0.0801, 0.0213, 0.0224, 0.0411, 0.4534, 0.4066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1432, 0.0476, 0.0122, 0.0368, 0.0413, 0.4470, 0.3366]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1745, 0.0155, 0.0252, 0.0161, 0.0148, 0.1308, 0.6099]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0536, 0.0253, 0.0340, 0.0416, 0.0351, 0.3477, 0.5472]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2449, 0.0920, 0.0115, 0.0152, 0.0562, 0.2193, 0.4611]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.2079, 0.1187, 0.0500, 0.0303, 0.0929, 0.2905, 0.3504]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0881, 0.0337, 0.1199, 0.0327, 0.0643, 0.1194, 0.3319]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0947, 0.1270, 0.0050, 0.0075, 0.0258, 0.2562, 0.3112]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1679, 0.0558, 0.0092, 0.0221, 0.0276, 0.3609, 0.5460]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[4.0841e-03, 1.7229e-03, 9.7810e-01, 8.7376e-05, 4.9267e-05, 1.7078e-04,\n",
      "         1.5885e-02]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1089, 0.0340, 0.0559, 0.0118, 0.0236, 0.2330, 0.3962]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0637, 0.0323, 0.0296, 0.0682, 0.0278, 0.2362, 0.4787]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0670, 0.0140, 0.0045, 0.0026, 0.0026, 0.1045, 0.6899]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1050, 0.1682, 0.0171, 0.0509, 0.0191, 0.2267, 0.2935]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([[0.1530, 0.0326, 0.0059, 0.0395, 0.0291, 0.2627, 0.4242]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 232\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# indexes = torch.argmax(outputs, dim=1)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# outputs = torch.zeros(outputs.shape, dtype=torch.float32).to(device)\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# outputs[torch.arange(indexes.size(0)), indexes] = 1\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m--> 232\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:503\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    495\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    496\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    502\u001b[0m     )\n\u001b[0;32m--> 503\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:254\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# data preparation \n",
    "train = pd.read_csv('data/new_train.csv', index_col='id')\n",
    "test = pd.read_csv('data/test.csv', index_col='id')\n",
    "\n",
    "y_columns = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "y = train[y_columns]\n",
    "X = train.drop(y_columns, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(test)\n",
    "y_test = np.zeros((X_test.shape[0], len(y_columns)))\n",
    "# y_test = pd.DataFrame(y_test, columns=y_columns)\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(y.values, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "X = X.view(-1, 1, 27)  # Reshape X for training data\n",
    "X_test = X_test.view(-1, 1, 27)  # Reshape X_test for testing data\n",
    "# X = X.view(-1, 27)  # Reshape X for training data\n",
    "# X_test = X_test.view(-1, 27)  # Reshape X_test for testing data\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for training and testing\n",
    "train_dataset = TensorDataset(X, y)\n",
    "test_dataset = TensorDataset(X_test, y_test)  # Assuming you have or will create y_test similarly\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# No need to shuffle the test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=7):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "        # Convolutional layers with increased dropout\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Increased dropout\n",
    "        self.conv2 = nn.Conv1d(64, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Increased dropout\n",
    "        self.conv3 = nn.Conv1d(256, 1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # Increased dropout\n",
    "        self.conv4 = nn.Conv1d(1024, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.dropout4 = nn.Dropout(0.5)  # Apply dropout also here\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self._to_linear = None\n",
    "        self._dummy_x = torch.zeros(1, input_channels, 27)\n",
    "        self._forward_features(self._dummy_x)\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        self.fc1 = nn.Linear(self._to_linear, 1024)\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)  # Apply dropout before final layer\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(x)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = int(x.numel() / x.size(0))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = self.dropout_fc1(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# class ResidualBlock1D(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "#         super(ResidualBlock1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         self.downsample = downsample\n",
    "#         self.dropout = nn.Dropout(0.5)  # Add dropout\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "        \n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.dropout(out)  # Apply dropout\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class ResNet1D(nn.Module):\n",
    "#     def __init__(self, input_channels=1, num_blocks=[2, 2, 2], num_classes=7):\n",
    "#         super(ResNet1D, self).__init__()\n",
    "#         self.in_channels = 64\n",
    "#         self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm1d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "#         self.layer1 = self._make_layer(64, num_blocks[0])\n",
    "#         self.layer2 = self._make_layer(128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(256, num_blocks[2], stride=2)\n",
    "        \n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "#     def _make_layer(self, out_channels, blocks, stride=1):\n",
    "#         downsample = None\n",
    "#         if stride != 1 or self.in_channels != out_channels:\n",
    "#             downsample = nn.Sequential(\n",
    "#                 nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm1d(out_channels),\n",
    "#             )\n",
    "        \n",
    "#         layers = []\n",
    "#         layers.append(ResidualBlock1D(self.in_channels, out_channels, stride, downsample))\n",
    "#         self.in_channels = out_channels  # Update in_channels to match out_channels for the next block\n",
    "#         for _ in range(1, blocks):\n",
    "#             layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        \n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "        \n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "        \n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         return torch.sigmoid(x)\n",
    "\n",
    "# class LargerFCNN(nn.Module):\n",
    "#     def __init__(self, input_dim=27, output_dim=7, dropout_rate=0.5):\n",
    "#         super(LargerFCNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, 256)  # First layer\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(256, 1024)  # Second layer\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(1024, 128)  # Third layer\n",
    "#         self.dropout3 = nn.Dropout(dropout_rate)\n",
    "#         self.fc4 = nn.Linear(128, 512)  # Fourth layer\n",
    "#         self.dropout4 = nn.Dropout(dropout_rate)\n",
    "#         self.fc5 = nn.Linear(512, output_dim)  # Output layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = self.dropout3(x)\n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.dropout4(x)\n",
    "#         x = torch.sigmoid(self.fc5(x))  # Sigmoid activation for binary output\n",
    "#         return x\n",
    "\n",
    "# Ensure model is compatible with CUDA\n",
    "model = DeeperCNN().to(device)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # outputs = outputs.detach().cpu().numpy()\n",
    "        # all_indexes = np.argmax(outputs, axis=1)\n",
    "        # outputs = np.zeros(outputs.shape)\n",
    "        # outputs[np.arange(all_indexes.size), all_indexes] = 1\n",
    "        # outputs = torch.tensor(outputs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        if epoch == 1: \n",
    "            print(outputs[0:1,:])\n",
    "        \n",
    "        # indexes = torch.argmax(outputs, dim=1)\n",
    "        # outputs = torch.zeros(outputs.shape, dtype=torch.float32).to(device)\n",
    "        # outputs[torch.arange(indexes.size(0)), indexes] = 1\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Scores for each output dimension: [0.6221756864332302, 0.7873114076237089, 0.9526771312397259, 0.9347323188514213, 0.6372189939357128, 0.7062049242231597, 0.6685828034555914]\n",
      "Mean AUC-ROC Score: 0.7584147522517928\n"
     ]
    }
   ],
   "source": [
    "# Store for predictions and actual labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# find the index of largest value in each row\n",
    "all_indexes = np.argmax(all_predictions, axis=1)\n",
    "all_predictions = np.zeros(all_predictions.shape)\n",
    "all_predictions[np.arange(all_indexes.size), all_indexes] = 1\n",
    "\n",
    "\n",
    "# Compute AUC-ROC for each dimension\n",
    "auc_scores = []\n",
    "for i in range(7):  # Assuming 7 output dimensions\n",
    "    auc_score = roc_auc_score(all_labels[:, i], all_predictions[:, i])\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "mean_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"AUC-ROC Scores for each output dimension:\", auc_scores)\n",
    "print(\"Mean AUC-ROC Score:\", mean_auc_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store for predictions and actual labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "all_indexes = np.argmax(all_predictions, axis=1)\n",
    "all_predictions = np.zeros(all_predictions.shape)\n",
    "all_predictions[np.arange(all_indexes.size), all_indexes] = 1\n",
    "\n",
    "# save to a file for submission\n",
    "# id starts at 19219\n",
    "submission = pd.DataFrame(all_predictions, columns=y_columns)\n",
    "submission.index += 19219\n",
    "submission.index.name = 'id'\n",
    "submission.to_csv('submissions/cnn7.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
