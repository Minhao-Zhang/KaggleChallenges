{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4E5 Regression with a Flood Prediction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "DATA_PATH = './data/'\n",
    "ORIGINAL_DATA_PATH = DATA_PATH\n",
    "SUBMISSIONS_PATH = './submissions/'\n",
    "MODELS_PATH = './trained_models/'\n",
    "TEMP_PATH = './temp/'\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv', index_col='id')\n",
    "test = pd.read_csv(DATA_PATH + 'test.csv', index_col='id')\n",
    "original = pd.read_csv(ORIGINAL_DATA_PATH + 'flood.csv')\n",
    "original.index.rename('id', inplace=True)\n",
    "new_train = pd.concat([train, original], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_FEATURES = ['FloodProbability', 'fold']\n",
    "BASE_FEATURES = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors']\n",
    "\n",
    "# meta features from https://www.kaggle.com/code/igorvolianiuk/flood-prediction-lgbm\n",
    "def add_features(df):\n",
    "    df['total'] = df[BASE_FEATURES].sum(axis=1)\n",
    "    df['amplified_sum'] = (df[BASE_FEATURES] ** 1.5).sum(axis=1)\n",
    "    df['fskew'] = df[BASE_FEATURES].skew(axis=1)\n",
    "    df['fkurtosis'] = df[BASE_FEATURES].kurtosis(axis=1)\n",
    "    df['mean'] = df[BASE_FEATURES].mean(axis=1)\n",
    "    df['std'] = df[BASE_FEATURES].std(axis=1)\n",
    "    df['max'] = df[BASE_FEATURES].max(axis=1)\n",
    "    df['min'] = df[BASE_FEATURES].min(axis=1)\n",
    "    df['range'] = df['max'] - df['min']\n",
    "    df['median'] = df[BASE_FEATURES].median(axis=1)\n",
    "    df['ptp'] = df[BASE_FEATURES].values.ptp(axis=1)\n",
    "    df['q25'] = df[BASE_FEATURES].quantile(0.25, axis=1)\n",
    "    df['q75'] = df[BASE_FEATURES].quantile(0.75, axis=1)\n",
    "    return df\n",
    "\n",
    "new_train = add_features(new_train)\n",
    "test = add_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Data Preparation\n",
    "X = new_train.drop('FloodProbability', axis=1)\n",
    "y = new_train['FloodProbability']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(test), columns=test.columns)\n",
    "\n",
    "# Convert the pandas DataFrame to numpy and then to torch Tensors\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_t = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_val_t = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "y_val_t = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "X_test_t = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "X_t = torch.tensor(X.values, dtype=torch.float32).to(device)\n",
    "y_t = torch.tensor(y.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader for the training data\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=2048, shuffle=True)\n",
    "all_train_loader = DataLoader(TensorDataset(X_t, y_t), batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0007724763127043843\n",
      "Epoch 2, Loss: 0.000412262073950842\n",
      "Epoch 3, Loss: 0.00043280154932290316\n",
      "Epoch 4, Loss: 0.0003684451512526721\n",
      "Epoch 5, Loss: 0.000402844074415043\n",
      "Epoch 6, Loss: 0.0003893839311785996\n",
      "Epoch 7, Loss: 0.00038644575397484004\n",
      "Epoch 8, Loss: 0.00034366524778306484\n",
      "Epoch 9, Loss: 0.0003396949323359877\n",
      "Epoch 10, Loss: 0.0003802865685429424\n",
      "Epoch 11, Loss: 0.0003342445124872029\n",
      "Epoch 12, Loss: 0.0003552164707798511\n",
      "Epoch 13, Loss: 0.0003413427621126175\n",
      "Epoch 14, Loss: 0.0003203046799171716\n",
      "Epoch 15, Loss: 0.00034489063546061516\n",
      "Epoch 16, Loss: 0.0003797139797825366\n",
      "Epoch 17, Loss: 0.00037352429353632033\n",
      "Epoch 18, Loss: 0.00037184380926191807\n",
      "Epoch 19, Loss: 0.0003514110576361418\n",
      "Epoch 20, Loss: 0.00038674272946082056\n",
      "Epoch 21, Loss: 0.000331233226461336\n",
      "Epoch 22, Loss: 0.0003847444895654917\n",
      "Epoch 23, Loss: 0.00037081766640767455\n",
      "Epoch 24, Loss: 0.00039203488267958164\n",
      "Epoch 25, Loss: 0.0003863824240397662\n",
      "Epoch 26, Loss: 0.00037658607470802963\n",
      "Epoch 27, Loss: 0.00033650590921752155\n",
      "Epoch 28, Loss: 0.0003457943967077881\n",
      "Epoch 29, Loss: 0.0003918127331417054\n",
      "Epoch 30, Loss: 0.0003880167205352336\n",
      "Epoch 31, Loss: 0.0003224433457944542\n",
      "Epoch 32, Loss: 0.0003426062467042357\n",
      "Epoch 33, Loss: 0.00036997589631937444\n",
      "Epoch 34, Loss: 0.0003463422763161361\n",
      "Epoch 35, Loss: 0.00031139550264924765\n",
      "Epoch 36, Loss: 0.0003610702697187662\n",
      "Epoch 37, Loss: 0.00032165570883080363\n",
      "Epoch 38, Loss: 0.0003882911114487797\n",
      "Epoch 39, Loss: 0.000368307635653764\n",
      "Epoch 40, Loss: 0.0003467011556494981\n",
      "Epoch 41, Loss: 0.00034228991717100143\n",
      "Epoch 42, Loss: 0.00030762384994886816\n",
      "Epoch 43, Loss: 0.00032508696313016117\n",
      "Epoch 44, Loss: 0.000333693198626861\n",
      "Epoch 45, Loss: 0.00030937534756958485\n",
      "Epoch 46, Loss: 0.0003435606777202338\n",
      "Epoch 47, Loss: 0.00035361613845452666\n",
      "Epoch 48, Loss: 0.0003402670845389366\n",
      "Epoch 49, Loss: 0.0003557972959242761\n",
      "Epoch 50, Loss: 0.0003356953093316406\n",
      "Epoch 51, Loss: 0.0003475658886600286\n",
      "Epoch 52, Loss: 0.0004043285152874887\n",
      "Epoch 53, Loss: 0.0003908226208295673\n",
      "Epoch 54, Loss: 0.0004112203896511346\n",
      "Epoch 55, Loss: 0.0003258321376051754\n",
      "Epoch 56, Loss: 0.0003741576219908893\n",
      "Epoch 57, Loss: 0.00034531537676230073\n",
      "Epoch 58, Loss: 0.0003550071269273758\n",
      "Epoch 59, Loss: 0.00037635830813087523\n",
      "Epoch 60, Loss: 0.00031960292835719883\n",
      "Epoch 61, Loss: 0.000336687546223402\n",
      "Epoch 62, Loss: 0.00034424077603034675\n",
      "Epoch 63, Loss: 0.00033119897125288844\n",
      "Epoch 64, Loss: 0.00032365048537030816\n",
      "Epoch 65, Loss: 0.00036598320002667606\n",
      "Epoch 66, Loss: 0.00040318063111044466\n",
      "Epoch 67, Loss: 0.0003653948660939932\n",
      "Epoch 68, Loss: 0.00036067934706807137\n",
      "Epoch 69, Loss: 0.0003840546414721757\n",
      "Epoch 70, Loss: 0.000353623297996819\n",
      "Epoch 71, Loss: 0.0003418054839130491\n",
      "Epoch 72, Loss: 0.00034347394830547273\n",
      "Epoch 73, Loss: 0.00035773857962340117\n",
      "Epoch 74, Loss: 0.0003423050802666694\n",
      "Epoch 75, Loss: 0.0003797740791924298\n",
      "Epoch 76, Loss: 0.00035215215757489204\n",
      "Epoch 77, Loss: 0.000311876239720732\n",
      "Epoch 78, Loss: 0.00032194479717873037\n",
      "Epoch 79, Loss: 0.00035476655466482043\n",
      "Epoch 80, Loss: 0.0003921091556549072\n",
      "Epoch 81, Loss: 0.00037692589103244245\n",
      "Epoch 82, Loss: 0.00033598768641240895\n",
      "Epoch 83, Loss: 0.00034814421087503433\n",
      "Epoch 84, Loss: 0.00034157573827542365\n",
      "Epoch 85, Loss: 0.0003337258822284639\n",
      "Epoch 86, Loss: 0.0003605429083108902\n",
      "Epoch 87, Loss: 0.0003200503997504711\n",
      "Epoch 88, Loss: 0.00036530010402202606\n",
      "Epoch 89, Loss: 0.0003631759318523109\n",
      "Epoch 90, Loss: 0.00035638859844766557\n",
      "Epoch 91, Loss: 0.0003188475966453552\n",
      "Epoch 92, Loss: 0.00038357084849849343\n",
      "Epoch 93, Loss: 0.0003705450508277863\n",
      "Epoch 94, Loss: 0.00035021171788685024\n",
      "Epoch 95, Loss: 0.0003509299422148615\n",
      "Epoch 96, Loss: 0.0003428728668950498\n",
      "Epoch 97, Loss: 0.00034113923902623355\n",
      "Epoch 98, Loss: 0.00035650524660013616\n",
      "Epoch 99, Loss: 0.00037227157736197114\n",
      "Epoch 100, Loss: 0.00032226089388132095\n",
      "Epoch 101, Loss: 0.0003708892618305981\n",
      "Epoch 102, Loss: 0.00033003196585923433\n",
      "Epoch 103, Loss: 0.000284489564364776\n",
      "Epoch 104, Loss: 0.0003404472372494638\n",
      "Epoch 105, Loss: 0.00029181657009758055\n",
      "Epoch 106, Loss: 0.0003516858851071447\n",
      "Epoch 107, Loss: 0.0003878326388075948\n",
      "Epoch 108, Loss: 0.00035408357507549226\n",
      "Epoch 109, Loss: 0.00032795395236462355\n",
      "Epoch 110, Loss: 0.00032028474379330873\n",
      "Epoch 111, Loss: 0.00034135222085751593\n",
      "Epoch 112, Loss: 0.00035100372042506933\n",
      "Epoch 113, Loss: 0.00039817075594328344\n",
      "Epoch 114, Loss: 0.0003625140816438943\n",
      "Epoch 115, Loss: 0.00033791750320233405\n",
      "Epoch 116, Loss: 0.0003322860866319388\n",
      "Epoch 117, Loss: 0.00033291266299784184\n",
      "Epoch 118, Loss: 0.0003176027094013989\n",
      "Epoch 119, Loss: 0.0003376248641870916\n",
      "Epoch 120, Loss: 0.00032325336360372603\n",
      "Epoch 121, Loss: 0.0003340311232022941\n",
      "Epoch 122, Loss: 0.00035526708234101534\n",
      "Epoch 123, Loss: 0.00044297642307356\n",
      "Epoch 124, Loss: 0.0003155427984893322\n",
      "Epoch 125, Loss: 0.00037534686271101236\n",
      "Epoch 126, Loss: 0.0003486672940198332\n",
      "Epoch 127, Loss: 0.00031274359207600355\n",
      "Epoch 128, Loss: 0.00037139025516808033\n",
      "Epoch 129, Loss: 0.0003478600992821157\n",
      "Epoch 130, Loss: 0.0003492925316095352\n",
      "Epoch 131, Loss: 0.0003507109940983355\n",
      "Epoch 132, Loss: 0.00032768608070909977\n",
      "Epoch 133, Loss: 0.0003518481971696019\n",
      "Epoch 134, Loss: 0.0003249703731853515\n",
      "Epoch 135, Loss: 0.00035634092637337744\n",
      "Epoch 136, Loss: 0.0003331000334583223\n",
      "Epoch 137, Loss: 0.0002942820719908923\n",
      "Epoch 138, Loss: 0.00034876243444159627\n",
      "Epoch 139, Loss: 0.0002889295283239335\n",
      "Epoch 140, Loss: 0.00033773554605431855\n",
      "Epoch 141, Loss: 0.00036707226536236703\n",
      "Epoch 142, Loss: 0.0003423458547331393\n",
      "Epoch 143, Loss: 0.00033618687302805483\n",
      "Epoch 144, Loss: 0.0003330891777295619\n",
      "Epoch 145, Loss: 0.00035229927743785083\n",
      "Epoch 146, Loss: 0.00036976614501327276\n",
      "Epoch 147, Loss: 0.0003886407648678869\n",
      "Epoch 148, Loss: 0.0003614624438341707\n",
      "Epoch 149, Loss: 0.0003490425006020814\n",
      "Epoch 150, Loss: 0.00031630927696824074\n",
      "Epoch 151, Loss: 0.0003693421313073486\n",
      "Epoch 152, Loss: 0.00036804104456678033\n",
      "Epoch 153, Loss: 0.0003374957013875246\n",
      "Epoch 154, Loss: 0.00032146016019396484\n",
      "Epoch 155, Loss: 0.0003579409676603973\n",
      "Epoch 156, Loss: 0.000361454498488456\n",
      "Epoch 157, Loss: 0.0003700252855196595\n",
      "Epoch 158, Loss: 0.00032924587139859796\n",
      "Epoch 159, Loss: 0.00033114224788732827\n",
      "Epoch 160, Loss: 0.00031842649332247674\n",
      "Epoch 161, Loss: 0.0003413492813706398\n",
      "Epoch 162, Loss: 0.0002867584116756916\n",
      "Epoch 163, Loss: 0.0003701004316098988\n",
      "Epoch 164, Loss: 0.00032024967367760837\n",
      "Epoch 165, Loss: 0.0003403389418963343\n",
      "Epoch 166, Loss: 0.00032534843194298446\n",
      "Epoch 167, Loss: 0.00034030471579171717\n",
      "Epoch 168, Loss: 0.0003736433864105493\n",
      "Epoch 169, Loss: 0.0003385455929674208\n",
      "Epoch 170, Loss: 0.00038136806688271463\n",
      "Epoch 171, Loss: 0.00035058739013038576\n",
      "Epoch 172, Loss: 0.0002969290071632713\n",
      "Epoch 173, Loss: 0.0003349913749843836\n",
      "Epoch 174, Loss: 0.0003503615444060415\n",
      "Epoch 175, Loss: 0.00031758658587932587\n",
      "Epoch 176, Loss: 0.00034326454624533653\n",
      "Epoch 177, Loss: 0.0004316086124163121\n",
      "Epoch 178, Loss: 0.00031602283706888556\n",
      "Epoch 179, Loss: 0.00033519419957883656\n",
      "Epoch 180, Loss: 0.0003223565290682018\n",
      "Epoch 181, Loss: 0.00037182681262493134\n",
      "Epoch 182, Loss: 0.00033433482167311013\n",
      "Epoch 183, Loss: 0.0003259145887568593\n",
      "Epoch 184, Loss: 0.00033177807927131653\n",
      "Epoch 185, Loss: 0.0003349755425006151\n",
      "Epoch 186, Loss: 0.0003064167976845056\n",
      "Epoch 187, Loss: 0.0003167512477375567\n",
      "Epoch 188, Loss: 0.0003002537414431572\n",
      "Epoch 189, Loss: 0.00035054763429798186\n",
      "Epoch 190, Loss: 0.0003553088172338903\n",
      "Epoch 191, Loss: 0.00034034086274914443\n",
      "Epoch 192, Loss: 0.0003250638546887785\n",
      "Epoch 193, Loss: 0.00038616155507043004\n",
      "Epoch 194, Loss: 0.0003830170026049018\n",
      "Epoch 195, Loss: 0.00033913401421159506\n",
      "Epoch 196, Loss: 0.00031741728889755905\n",
      "Epoch 197, Loss: 0.0003285205166321248\n",
      "Epoch 198, Loss: 0.000291614793241024\n",
      "Epoch 199, Loss: 0.0003505368949845433\n",
      "Epoch 200, Loss: 0.0003244639083277434\n",
      "Epoch 201, Loss: 0.0003217185731045902\n",
      "Epoch 202, Loss: 0.0003374891821295023\n",
      "Epoch 203, Loss: 0.00031175147159956396\n",
      "Epoch 204, Loss: 0.0003066322533413768\n",
      "Epoch 205, Loss: 0.0003695749619510025\n",
      "Epoch 206, Loss: 0.0003432075900491327\n",
      "Epoch 207, Loss: 0.00034712310298345983\n",
      "Epoch 208, Loss: 0.00032601470593363047\n",
      "Epoch 209, Loss: 0.0003702347166836262\n",
      "Epoch 210, Loss: 0.00030985433841124177\n",
      "Epoch 211, Loss: 0.0003547939413692802\n",
      "Epoch 212, Loss: 0.00037393628736026585\n",
      "Epoch 213, Loss: 0.0003465667541604489\n",
      "Epoch 214, Loss: 0.00037414030521176755\n",
      "Epoch 215, Loss: 0.0003343967837281525\n",
      "Epoch 216, Loss: 0.00038040123763494194\n",
      "Epoch 217, Loss: 0.0003481695894151926\n",
      "Epoch 218, Loss: 0.00031838519498705864\n",
      "Epoch 219, Loss: 0.00032785453367978334\n",
      "Epoch 220, Loss: 0.0003634565800894052\n",
      "Epoch 221, Loss: 0.00033630189136601985\n",
      "Epoch 222, Loss: 0.00035041049704886973\n",
      "Epoch 223, Loss: 0.00034091409179382026\n",
      "Epoch 224, Loss: 0.000311235839035362\n",
      "Epoch 225, Loss: 0.0003248901048209518\n",
      "Epoch 226, Loss: 0.0003539476019795984\n",
      "Epoch 227, Loss: 0.0003530172980390489\n",
      "Epoch 228, Loss: 0.000327377172652632\n",
      "Epoch 229, Loss: 0.00036111610825173557\n",
      "Epoch 230, Loss: 0.000315492128720507\n",
      "Epoch 231, Loss: 0.00036554571124725044\n",
      "Epoch 232, Loss: 0.0003657490306068212\n",
      "Epoch 233, Loss: 0.0002945524174720049\n",
      "Epoch 234, Loss: 0.00027585806674323976\n",
      "Epoch 235, Loss: 0.00036143852048553526\n",
      "Epoch 236, Loss: 0.00032840901985764503\n",
      "Epoch 237, Loss: 0.00032537520746700466\n",
      "Epoch 238, Loss: 0.00032871071016415954\n",
      "Epoch 239, Loss: 0.00032875529723241925\n",
      "Epoch 240, Loss: 0.00035070665762759745\n",
      "Epoch 241, Loss: 0.0003784691507462412\n",
      "Epoch 242, Loss: 0.00032163472496904433\n",
      "Epoch 243, Loss: 0.00032061507226899266\n",
      "Epoch 244, Loss: 0.0003151544078718871\n",
      "Epoch 245, Loss: 0.0003506135253701359\n",
      "Epoch 246, Loss: 0.000316527730319649\n",
      "Epoch 247, Loss: 0.00042235670844092965\n",
      "Epoch 248, Loss: 0.00034672519541345537\n",
      "Epoch 249, Loss: 0.00036241867928765714\n",
      "Epoch 250, Loss: 0.00036917568650096655\n",
      "Epoch 251, Loss: 0.0003442687157075852\n",
      "Epoch 252, Loss: 0.0003779789840336889\n",
      "Epoch 253, Loss: 0.0003673227911349386\n",
      "Epoch 254, Loss: 0.00034461781615391374\n",
      "Epoch 255, Loss: 0.00032410453422926366\n",
      "Epoch 256, Loss: 0.00038178401882760227\n",
      "Epoch 257, Loss: 0.0003763971908483654\n",
      "Epoch 258, Loss: 0.0003534020797815174\n",
      "Epoch 259, Loss: 0.00033238279866054654\n",
      "Epoch 260, Loss: 0.0003084897471126169\n",
      "Epoch 261, Loss: 0.0003404172894079238\n",
      "Epoch 262, Loss: 0.0003487729118205607\n",
      "Epoch 263, Loss: 0.0003512453695293516\n",
      "Epoch 264, Loss: 0.00030089152278378606\n",
      "Epoch 265, Loss: 0.0003538963501341641\n",
      "Epoch 266, Loss: 0.00035594854853115976\n",
      "Epoch 267, Loss: 0.00035666421172209084\n",
      "Epoch 268, Loss: 0.00035790842957794666\n",
      "Epoch 269, Loss: 0.000320635037496686\n",
      "Epoch 270, Loss: 0.0003423193993512541\n",
      "Epoch 271, Loss: 0.0003254732582718134\n",
      "Epoch 272, Loss: 0.0003510297683533281\n",
      "Epoch 273, Loss: 0.00037000939482823014\n",
      "Epoch 274, Loss: 0.0003733795019797981\n",
      "Epoch 275, Loss: 0.00032040695077739656\n",
      "Epoch 276, Loss: 0.00036843688576482236\n",
      "Epoch 277, Loss: 0.00036960653960704803\n",
      "Epoch 278, Loss: 0.0003840117424260825\n",
      "Epoch 279, Loss: 0.0003373889485374093\n",
      "Epoch 280, Loss: 0.0003518169978633523\n",
      "Epoch 281, Loss: 0.00033974688267335296\n",
      "Epoch 282, Loss: 0.00031903921626508236\n",
      "Epoch 283, Loss: 0.00034330663038417697\n",
      "Epoch 284, Loss: 0.00030884885927662253\n",
      "Epoch 285, Loss: 0.00036176046705804765\n",
      "Epoch 286, Loss: 0.00032877313788048923\n",
      "Epoch 287, Loss: 0.0003434167883824557\n",
      "Epoch 288, Loss: 0.00038468610728159547\n",
      "Epoch 289, Loss: 0.0003412545775063336\n",
      "Epoch 290, Loss: 0.00033811942557804286\n",
      "Epoch 291, Loss: 0.00031646338175050914\n",
      "Epoch 292, Loss: 0.0003425561881158501\n",
      "Epoch 293, Loss: 0.0003572950081434101\n",
      "Epoch 294, Loss: 0.00031285284785553813\n",
      "Epoch 295, Loss: 0.00034515970037318766\n",
      "Epoch 296, Loss: 0.00033567080390639603\n",
      "Epoch 297, Loss: 0.0003142486675642431\n",
      "Epoch 298, Loss: 0.00032241191365756094\n",
      "Epoch 299, Loss: 0.000350850255927071\n",
      "Epoch 300, Loss: 0.00034744213917292655\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Define the MLP Model\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=X_train.shape[1]):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Adjust the input dimension dynamically\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n",
    "\n",
    "mlp_model = MLPRegressor().to(device)\n",
    "\n",
    "### Training Parameters\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "### Training Loop\n",
    "num_epochs = 300\n",
    "mlp_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data, targets in all_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(data)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "torch.save(mlp_model, MODELS_PATH + 'mlp5.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "# mlp_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     train_predictions = mlp_model(X_train_t).view(-1)\n",
    "#     score = r2_score(y_train, train_predictions.cpu())\n",
    "#     print(f'Training R2 Score: {score}')\n",
    "#     predictions = mlp_model(X_val_t).view(-1)\n",
    "#     score = r2_score(y_val, predictions.cpu())\n",
    "#     print(f'Validating R2 Score: {score}')\n",
    "    \n",
    "#     tmp = pd.DataFrame({'id': X_val.index, 'FloodProbability': predictions.cpu()})\n",
    "#     tmp.to_csv(TEMP_PATH + 'mlp_val.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = mlp_model(X_test_t).view(-1)\n",
    "    submission = pd.DataFrame({'id': test.index, 'FloodProbability': predictions.cpu()})\n",
    "    submission.to_csv(SUBMISSIONS_PATH + 'mlp5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00037581182550638914\n",
      "Epoch 2, Loss: 0.00038602156564593315\n",
      "Epoch 3, Loss: 0.00036227694363333285\n",
      "Epoch 4, Loss: 0.0004159337258897722\n",
      "Epoch 5, Loss: 0.00035183419822715223\n",
      "Epoch 6, Loss: 0.0003638999769464135\n",
      "Epoch 7, Loss: 0.0003570918634068221\n",
      "Epoch 8, Loss: 0.0003477736026979983\n",
      "Epoch 9, Loss: 0.0004024760564789176\n",
      "Epoch 10, Loss: 0.00039086255128495395\n",
      "Epoch 11, Loss: 0.0003162657667417079\n",
      "Epoch 12, Loss: 0.0003838306583929807\n",
      "Epoch 13, Loss: 0.0004138789081480354\n",
      "Epoch 14, Loss: 0.000355051044607535\n",
      "Epoch 15, Loss: 0.0004736946430057287\n",
      "Epoch 16, Loss: 0.00035088456934317946\n",
      "Epoch 17, Loss: 0.00041215901728719473\n",
      "Epoch 18, Loss: 0.00038932295865379274\n",
      "Epoch 19, Loss: 0.0004177643568255007\n",
      "Epoch 20, Loss: 0.0003422682930249721\n",
      "Epoch 21, Loss: 0.00038642046274617314\n",
      "Epoch 22, Loss: 0.0004015204613097012\n",
      "Epoch 23, Loss: 0.0003421467263251543\n",
      "Epoch 24, Loss: 0.0003769267932511866\n",
      "Epoch 25, Loss: 0.0003298541996628046\n",
      "Epoch 26, Loss: 0.0003442151937633753\n",
      "Epoch 27, Loss: 0.00038190573104657233\n",
      "Epoch 28, Loss: 0.0003562777128536254\n",
      "Epoch 29, Loss: 0.0003422006848268211\n",
      "Epoch 30, Loss: 0.000365055981092155\n",
      "Epoch 31, Loss: 0.00036182350595481694\n",
      "Epoch 32, Loss: 0.00031486438820138574\n",
      "Epoch 33, Loss: 0.0003675513726193458\n",
      "Epoch 34, Loss: 0.00036433932837098837\n",
      "Epoch 35, Loss: 0.0003633093147072941\n",
      "Epoch 36, Loss: 0.00031771804788149893\n",
      "Epoch 37, Loss: 0.0003592623397707939\n",
      "Epoch 38, Loss: 0.00035949843004345894\n",
      "Epoch 39, Loss: 0.0003797342942561954\n",
      "Epoch 40, Loss: 0.0003698533691931516\n",
      "Epoch 41, Loss: 0.0003221248334739357\n",
      "Epoch 42, Loss: 0.0003803892759606242\n",
      "Epoch 43, Loss: 0.00033225707011297345\n",
      "Epoch 44, Loss: 0.00030768581200391054\n",
      "Epoch 45, Loss: 0.00031905373907648027\n",
      "Epoch 46, Loss: 0.00035769082023762167\n",
      "Epoch 47, Loss: 0.0003143895883113146\n",
      "Epoch 48, Loss: 0.00031898386077955365\n",
      "Epoch 49, Loss: 0.0003194998134858906\n",
      "Epoch 50, Loss: 0.00033230893313884735\n",
      "Epoch 51, Loss: 0.0003560475306585431\n",
      "Epoch 52, Loss: 0.00037951889680698514\n",
      "Epoch 53, Loss: 0.00030954513931646943\n",
      "Epoch 54, Loss: 0.0003700254892464727\n",
      "Epoch 55, Loss: 0.0003940260212402791\n",
      "Epoch 56, Loss: 0.0003567107778508216\n",
      "Epoch 57, Loss: 0.0003090755781158805\n",
      "Epoch 58, Loss: 0.00035565905272960663\n",
      "Epoch 59, Loss: 0.0003352054918650538\n",
      "Epoch 60, Loss: 0.0003321609110571444\n",
      "Epoch 61, Loss: 0.00037458055885508657\n",
      "Epoch 62, Loss: 0.000368786248145625\n",
      "Epoch 63, Loss: 0.00034422564203850925\n",
      "Epoch 64, Loss: 0.00034343922743573785\n",
      "Epoch 65, Loss: 0.0003461373853497207\n",
      "Epoch 66, Loss: 0.00031159751233644783\n",
      "Epoch 67, Loss: 0.0003417491680011153\n",
      "Epoch 68, Loss: 0.0003548061358742416\n",
      "Epoch 69, Loss: 0.0003483129548840225\n",
      "Epoch 70, Loss: 0.0003130060504190624\n",
      "Epoch 71, Loss: 0.0003366479359101504\n",
      "Epoch 72, Loss: 0.000389524589991197\n",
      "Epoch 73, Loss: 0.00040783677832223475\n",
      "Epoch 74, Loss: 0.0003354343061801046\n",
      "Epoch 75, Loss: 0.00034401658922433853\n",
      "Epoch 76, Loss: 0.0003608562401495874\n",
      "Epoch 77, Loss: 0.0003679891233332455\n",
      "Epoch 78, Loss: 0.00037193551543168724\n",
      "Epoch 79, Loss: 0.00035216944525018334\n",
      "Epoch 80, Loss: 0.0003566767554730177\n",
      "Epoch 81, Loss: 0.0003527907538227737\n",
      "Epoch 82, Loss: 0.0003364908916410059\n",
      "Epoch 83, Loss: 0.00037927672383375466\n",
      "Epoch 84, Loss: 0.00038332989788614213\n",
      "Epoch 85, Loss: 0.00038065537228249013\n",
      "Epoch 86, Loss: 0.0003141552151646465\n",
      "Epoch 87, Loss: 0.0003263662219978869\n",
      "Epoch 88, Loss: 0.0003241870435886085\n",
      "Epoch 89, Loss: 0.0003503890475258231\n",
      "Epoch 90, Loss: 0.00029443102539516985\n",
      "Epoch 91, Loss: 0.00037156828329898417\n",
      "Epoch 92, Loss: 0.0003603928198572248\n",
      "Epoch 93, Loss: 0.0003209227288607508\n",
      "Epoch 94, Loss: 0.000328541558701545\n",
      "Epoch 95, Loss: 0.0003516336146276444\n",
      "Epoch 96, Loss: 0.00036628011730499566\n",
      "Epoch 97, Loss: 0.0003590914129745215\n",
      "Epoch 98, Loss: 0.0003827742475550622\n",
      "Epoch 99, Loss: 0.0003448956704232842\n",
      "Epoch 100, Loss: 0.00032556045334786177\n",
      "Epoch 101, Loss: 0.0003339861868880689\n",
      "Epoch 102, Loss: 0.00036712567089125514\n",
      "Epoch 103, Loss: 0.00036880673724226654\n",
      "Epoch 104, Loss: 0.00033769421861507\n",
      "Epoch 105, Loss: 0.0003330302133690566\n",
      "Epoch 106, Loss: 0.0003959979221690446\n",
      "Epoch 107, Loss: 0.00035596286761574447\n",
      "Epoch 108, Loss: 0.00035380639019422233\n",
      "Epoch 109, Loss: 0.0003347444289829582\n",
      "Epoch 110, Loss: 0.00032386562088504434\n",
      "Epoch 111, Loss: 0.00033223582431674004\n",
      "Epoch 112, Loss: 0.0003119583707302809\n",
      "Epoch 113, Loss: 0.00034019144368357956\n",
      "Epoch 114, Loss: 0.00031183683313429356\n",
      "Epoch 115, Loss: 0.000326792272971943\n",
      "Epoch 116, Loss: 0.0003655482141766697\n",
      "Epoch 117, Loss: 0.00035118890809826553\n",
      "Epoch 118, Loss: 0.00033264822559431195\n",
      "Epoch 119, Loss: 0.0003060377493966371\n",
      "Epoch 120, Loss: 0.0003743740380741656\n",
      "Epoch 121, Loss: 0.0003254170005675405\n",
      "Epoch 122, Loss: 0.0003505796776153147\n",
      "Epoch 123, Loss: 0.0003551467671059072\n",
      "Epoch 124, Loss: 0.0003717491345014423\n",
      "Epoch 125, Loss: 0.0002892107586376369\n",
      "Epoch 126, Loss: 0.0003551397821865976\n",
      "Epoch 127, Loss: 0.00033983198227360845\n",
      "Epoch 128, Loss: 0.00031794121605344117\n",
      "Epoch 129, Loss: 0.0003174883022438735\n",
      "Epoch 130, Loss: 0.0003620597708504647\n",
      "Epoch 131, Loss: 0.0003189455019310117\n",
      "Epoch 132, Loss: 0.00032835188903845847\n",
      "Epoch 133, Loss: 0.0003992767306044698\n",
      "Epoch 134, Loss: 0.0003203652158845216\n",
      "Epoch 135, Loss: 0.0003508521185722202\n",
      "Epoch 136, Loss: 0.0003230239381082356\n",
      "Epoch 137, Loss: 0.0003799079859163612\n",
      "Epoch 138, Loss: 0.0003561565827112645\n",
      "Epoch 139, Loss: 0.00037073041312396526\n",
      "Epoch 140, Loss: 0.00032038515200838447\n",
      "Epoch 141, Loss: 0.0003374809166416526\n",
      "Epoch 142, Loss: 0.0003098660381510854\n",
      "Epoch 143, Loss: 0.00034993485314771533\n",
      "Epoch 144, Loss: 0.000296315411105752\n",
      "Epoch 145, Loss: 0.00031732433126308024\n",
      "Epoch 146, Loss: 0.0003565640654414892\n",
      "Epoch 147, Loss: 0.00037730048643425107\n",
      "Epoch 148, Loss: 0.0003258910437580198\n",
      "Epoch 149, Loss: 0.00036973567330278456\n",
      "Epoch 150, Loss: 0.00032747548539191484\n",
      "Epoch 151, Loss: 0.00030624945065937936\n",
      "Epoch 152, Loss: 0.000352618342731148\n",
      "Epoch 153, Loss: 0.0003521270991768688\n",
      "Epoch 154, Loss: 0.00034348975168541074\n",
      "Epoch 155, Loss: 0.000385698804166168\n",
      "Epoch 156, Loss: 0.00037121112109161913\n",
      "Epoch 157, Loss: 0.0003243967657908797\n",
      "Epoch 158, Loss: 0.0003575913142412901\n",
      "Epoch 159, Loss: 0.00030753854662179947\n",
      "Epoch 160, Loss: 0.00034558208426460624\n",
      "Epoch 161, Loss: 0.0003100901667494327\n",
      "Epoch 162, Loss: 0.00038978131487965584\n",
      "Epoch 163, Loss: 0.0003817410906776786\n",
      "Epoch 164, Loss: 0.00035583972930908203\n",
      "Epoch 165, Loss: 0.00034778876579366624\n",
      "Epoch 166, Loss: 0.0003450292570050806\n",
      "Epoch 167, Loss: 0.000351352384313941\n",
      "Epoch 168, Loss: 0.0003384192823432386\n",
      "Epoch 169, Loss: 0.00036975572584196925\n",
      "Epoch 170, Loss: 0.00033405786962248385\n",
      "Epoch 171, Loss: 0.0003328308230265975\n",
      "Epoch 172, Loss: 0.00031263503478839993\n",
      "Epoch 173, Loss: 0.00036644042120315135\n",
      "Epoch 174, Loss: 0.0003484021872282028\n",
      "Epoch 175, Loss: 0.0003257252974435687\n",
      "Epoch 176, Loss: 0.0003545658546499908\n",
      "Epoch 177, Loss: 0.00034742304706014693\n",
      "Epoch 178, Loss: 0.0003496288845781237\n",
      "Epoch 179, Loss: 0.0003380879934411496\n",
      "Epoch 180, Loss: 0.0003225404070690274\n",
      "Epoch 181, Loss: 0.00036943628219887614\n",
      "Epoch 182, Loss: 0.000325394474202767\n",
      "Epoch 183, Loss: 0.00035996048245579004\n",
      "Epoch 184, Loss: 0.0003263619728386402\n",
      "Epoch 185, Loss: 0.00035171766649000347\n",
      "Epoch 186, Loss: 0.0003575978334993124\n",
      "Epoch 187, Loss: 0.0003249888541176915\n",
      "Epoch 188, Loss: 0.0003574417787604034\n",
      "Epoch 189, Loss: 0.00033843802521005273\n",
      "Epoch 190, Loss: 0.00033047195756807923\n",
      "Epoch 191, Loss: 0.00033928040647879243\n",
      "Epoch 192, Loss: 0.00031184396357275546\n",
      "Epoch 193, Loss: 0.00035273158573545516\n",
      "Epoch 194, Loss: 0.0003493363910820335\n",
      "Epoch 195, Loss: 0.00032949892920441926\n",
      "Epoch 196, Loss: 0.0003477871068753302\n",
      "Epoch 197, Loss: 0.0003280106175225228\n",
      "Epoch 198, Loss: 0.0003785334702115506\n",
      "Epoch 199, Loss: 0.00033215791336260736\n",
      "Epoch 200, Loss: 0.000346992164850235\n",
      "Epoch 201, Loss: 0.00037573836743831635\n",
      "Epoch 202, Loss: 0.0003896034322679043\n",
      "Epoch 203, Loss: 0.00034222955582663417\n",
      "Epoch 204, Loss: 0.0003375841479282826\n",
      "Epoch 205, Loss: 0.00036836345680058\n",
      "Epoch 206, Loss: 0.0003287850704509765\n",
      "Epoch 207, Loss: 0.00035189787740819156\n",
      "Epoch 208, Loss: 0.0003618430346250534\n",
      "Epoch 209, Loss: 0.0003579727199394256\n",
      "Epoch 210, Loss: 0.00033671155688352883\n",
      "Epoch 211, Loss: 0.00033836791408248246\n",
      "Epoch 212, Loss: 0.0003805391024798155\n",
      "Epoch 213, Loss: 0.0003523811756167561\n",
      "Epoch 214, Loss: 0.0003303432313259691\n",
      "Epoch 215, Loss: 0.00031591628794558346\n",
      "Epoch 216, Loss: 0.0003395494422875345\n",
      "Epoch 217, Loss: 0.0003490390081424266\n",
      "Epoch 218, Loss: 0.0003344480646774173\n",
      "Epoch 219, Loss: 0.0003474636177998036\n",
      "Epoch 220, Loss: 0.0003645098186098039\n",
      "Epoch 221, Loss: 0.00033598198206163943\n",
      "Epoch 222, Loss: 0.00029129619360901415\n",
      "Epoch 223, Loss: 0.0003106703807134181\n",
      "Epoch 224, Loss: 0.0003492773394100368\n",
      "Epoch 225, Loss: 0.00035994371864944696\n",
      "Epoch 226, Loss: 0.00034090696135535836\n",
      "Epoch 227, Loss: 0.00033593131229281425\n",
      "Epoch 228, Loss: 0.0003567574603948742\n",
      "Epoch 229, Loss: 0.00031997813493944705\n",
      "Epoch 230, Loss: 0.0003378491965122521\n",
      "Epoch 231, Loss: 0.0003343707649037242\n",
      "Epoch 232, Loss: 0.00031514218426309526\n",
      "Epoch 233, Loss: 0.0003496553690638393\n",
      "Epoch 234, Loss: 0.00031246207072399557\n",
      "Epoch 235, Loss: 0.00031487748492509127\n",
      "Epoch 236, Loss: 0.0003610932326409966\n",
      "Epoch 237, Loss: 0.0003332932246848941\n",
      "Epoch 238, Loss: 0.00036424948484636843\n",
      "Epoch 239, Loss: 0.00034146697726100683\n",
      "Epoch 240, Loss: 0.00032155861845239997\n",
      "Epoch 241, Loss: 0.00032035497133620083\n",
      "Epoch 242, Loss: 0.00032648659544065595\n",
      "Epoch 243, Loss: 0.00033434052602387965\n",
      "Epoch 244, Loss: 0.0003093848063144833\n",
      "Epoch 245, Loss: 0.0003989181132055819\n",
      "Epoch 246, Loss: 0.00033064771560020745\n",
      "Epoch 247, Loss: 0.0003296378126833588\n",
      "Epoch 248, Loss: 0.00036535883555188775\n",
      "Epoch 249, Loss: 0.0003694204497151077\n",
      "Epoch 250, Loss: 0.00036080318386666477\n",
      "Epoch 251, Loss: 0.00034280691761523485\n",
      "Epoch 252, Loss: 0.00033377858926542103\n",
      "Epoch 253, Loss: 0.00032685999758541584\n",
      "Epoch 254, Loss: 0.00032940745586529374\n",
      "Epoch 255, Loss: 0.0003705115523189306\n",
      "Epoch 256, Loss: 0.0003182422660756856\n",
      "Epoch 257, Loss: 0.0003687668067868799\n",
      "Epoch 258, Loss: 0.0003419351123739034\n",
      "Epoch 259, Loss: 0.00032940736855380237\n",
      "Epoch 260, Loss: 0.00032384932273998857\n",
      "Epoch 261, Loss: 0.0003817849210463464\n",
      "Epoch 262, Loss: 0.0003154604637529701\n",
      "Epoch 263, Loss: 0.0003506127977743745\n",
      "Epoch 264, Loss: 0.0003284756967332214\n",
      "Epoch 265, Loss: 0.0003308575542178005\n",
      "Epoch 266, Loss: 0.00034882983891293406\n",
      "Epoch 267, Loss: 0.0003912164829671383\n",
      "Epoch 268, Loss: 0.0003295184869784862\n",
      "Epoch 269, Loss: 0.0003840129647869617\n",
      "Epoch 270, Loss: 0.00038547246367670596\n",
      "Epoch 271, Loss: 0.0003552911803126335\n",
      "Epoch 272, Loss: 0.00039511086652055383\n",
      "Epoch 273, Loss: 0.00032306069624610245\n",
      "Epoch 274, Loss: 0.0003254298644606024\n",
      "Epoch 275, Loss: 0.0003308540617581457\n",
      "Epoch 276, Loss: 0.00035255769034847617\n",
      "Epoch 277, Loss: 0.00039498749538324773\n",
      "Epoch 278, Loss: 0.000333791394950822\n",
      "Epoch 279, Loss: 0.0003051297680940479\n",
      "Epoch 280, Loss: 0.0003085564821958542\n",
      "Epoch 281, Loss: 0.00033950561191886663\n",
      "Epoch 282, Loss: 0.00035158428363502026\n",
      "Epoch 283, Loss: 0.0003288709558546543\n",
      "Epoch 284, Loss: 0.0003565632796380669\n",
      "Epoch 285, Loss: 0.00028935872251167893\n",
      "Epoch 286, Loss: 0.00032422857475467026\n",
      "Epoch 287, Loss: 0.0003246543346904218\n",
      "Epoch 288, Loss: 0.0003270210581831634\n",
      "Epoch 289, Loss: 0.00034839988802559674\n",
      "Epoch 290, Loss: 0.00035269284853711724\n",
      "Epoch 291, Loss: 0.00032736320281401277\n",
      "Epoch 292, Loss: 0.00031370780197903514\n",
      "Epoch 293, Loss: 0.00035616994136944413\n",
      "Epoch 294, Loss: 0.0003330773033667356\n",
      "Epoch 295, Loss: 0.0003188749251421541\n",
      "Epoch 296, Loss: 0.0003372602805029601\n",
      "Epoch 297, Loss: 0.0003388780460227281\n",
      "Epoch 298, Loss: 0.00032342513441108167\n",
      "Epoch 299, Loss: 0.0003324840508867055\n",
      "Epoch 300, Loss: 0.0003271684981882572\n"
     ]
    }
   ],
   "source": [
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=X_train.shape[1]):\n",
    "        super(CNNRegressor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)  # Input channels, output channels, kernel size\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * input_dim, 128)  # Adjust size based on output of last conv layer\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension here in the forward pass\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "cnn_model = CNNRegressor().to(device)\n",
    "\n",
    "# Training Parameters\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, targets in all_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(data)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "torch.save(cnn_model, MODELS_PATH + 'cnn5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation\n",
    "# cnn_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     train_predictions = cnn_model(X_train_t).view(-1)\n",
    "#     score = r2_score(y_train, train_predictions.cpu())\n",
    "#     print(f'Training R2 Score: {score}')\n",
    "#     predictions = cnn_model(X_val_t).view(-1)\n",
    "#     score = r2_score(y_val, predictions.cpu())\n",
    "#     print(f'Validating R2 Score: {score}')\n",
    "    \n",
    "#     tmp = pd.DataFrame({'id': X_val.index, 'FloodProbability': predictions.cpu()})\n",
    "#     tmp.to_csv(TEMP_PATH + 'cnn_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = cnn_model(X_test_t).view(-1)\n",
    "    submission = pd.DataFrame({'id': test.index, 'FloodProbability': predictions.cpu()})\n",
    "    submission.to_csv(SUBMISSIONS_PATH + 'cnn5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
